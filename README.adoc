:doctype: book
:toc: left
:toclevels: 3
:toc: macro
:sectnums:
:icons: font
:source-highlighter: highlightjs
:idprefix:
:idseparator: -
:sectlinks:
:sectanchors:
:linkcss: false

// Vars
:project-group-id: org.bonitasoft.connectors
:project-artifact-id: bonita-connector-ai
:project-version: 0.0.1-SNAPSHOT
:orga: bonitasoft
:uri-org: https://github.com/{orga}
:uri-repo: {uri-org}/{project-artifact-id}
:short-bonita-version: 10.2
:year-bonita-version: 2024.3
:doc-url: https://documentation.bonitasoft.com/bonita/{short-bonita-version}
:java-version: 17
= {project-artifact-id}

image:bonitasoft-community.png[Bonitasoft,link="https://www.bonitasoft.com",width=250px]
image:./openai@2x.png[OpenAI,link="https://openai.com"] image:./mistralai@2x.png[MistralAI,link="https://mistral.ai"]

image:{uri-repo}/actions/workflows/build.yaml/badge.svg[Build,link="{uri-repo}/actions?query=build"]

The connectors allow to interact with OpenAI and MistralAI chat models by sending prompt and documents and returning the generated output.

The Bonita AI Connectors are available for **Bonita {short-bonita-version} Community ({year-bonita-version})** version and above.

[WARNING]
====
The connector is still under development and current state is just a draft version.
====

toc::[]

== Getting started

To use a connector, add it as a dependency to your Bonita process. Choose the one related to your AI provider.

Currently supported providers:

* image:bonita-connector-ai-openai/src/main/resources/openai.png[OpenAI] OpenAI
* image:bonita-connector-ai-mistral/src/main/resources/mistral.png[MistralAI] Mistral AI

==== OpenAI

[source,xml,subs="attributes+"]
----
<dependency>
    <groupId>org.bonitasoft.connectors</groupId>
    <artifactId>bonita-connector-ai-openai</artifactId>
    <version>{project-version}</version>
</dependency>
----

==== MistralAI

WARNING: Image documents are not supported yet for Mistral connector due to a limitation of the underlying library we use.

[source,xml,subs="attributes+"]
----
<dependency>
    <groupId>org.bonitasoft.connectors</groupId>
    <artifactId>bonita-connector-ai-mistral</artifactId>
    <version>{project-version}</version>
</dependency>
----

=== Common configuration

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*apiKey*
|false
|The AI provider API key. Parameter is optional for testing purpose but obviously required with official OpenAI endpoint. The connector will use the system environment variable named `AI_API_KEY` or the same JVM property (`-DAI_API_KEY=xxx`) or the provided connector parameter and at last a dummy `changeMe` value.
| changeMe

|*url*
|false
|The OpenAI endpoint url. This parameter allows to use an alternate endpoint for tests.
|Default to the official provider endpoint if not specified.

|*requestTimeout*
|false
|The request timeout in milliseconds for OpenAI calls.
|null

|*chatModelName*
|false
|The model to use for chat like `gpt-4o`, `gpt-4o-mini`, `llama`, `pixtral`, ... See OpenAI documentation at https://platform.openai.com/docs/models/models and Mistral AI documentation at https://docs.mistral.ai/getting-started/models/models_overview/ for detailed information about models.
a|
- OpenAI : `gpt-4o`
- MistralAI : `pixtral-12b-2409`

|*modelTemperature*
|false
|The temperature to use for the model. Higher values will result in more creative responses. Must be between 0 and 1. Leave blank if the selected model does not support this parameter. If parameter is not present, the temperature will not be set in chat context.
|null
|===


== Connectors

AI connectors have the capability to retrun structured data in JSON format. It is possible to pass a https://json-schema.org/learn/getting-started-step-by-step[JSON schema] to tell the LLM how to format response data.

When using a JSON schema, you **must** list in the `required` property, all the fields you want in the JSON response.

.JSON schema sample
[source, json]
----
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "ProofOfAddress",
  "type": "object",
  "required": [
    "firstName",
    "lastName",
    "fullName",
    "fullAddress",
    "emissionDate",
    "issuerName",
    "identificationNumber"
  ],
  "properties": {
    "firstName": {
      "type": "string"
    },
    "lastName": {
      "type": "string"
    },
    "fullName": {
      "type": "string"
    },
    "fullAddress": {
      "type": "string"
    },
    "emissionDate": {
      "type": "string"
    },
    "issuerName": {
      "type": "string"
    },
    "identificationNumber": {
      "type": "string"
    }
  }
}
----

=== Ask connector

Take a user prompt and send it to OpenAI then return the AI response. The prompt text can ask question about a provided process document.

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*systemPrompt*
|false
|The system prompt to pass to the OpenAI endpoint.  It helps to influence the behavior of the assistant and specify a default context. (ex: You are a personal finance advisor, providing guidance, etc.)
|"You are a polite Assistant"

|*userPrompt*
|*true*
|The user prompt content to send to the AI provider
|

|*sourceDocumentRef*
|false
|The reference to the process document to load and add to the user prompt. If not null, the connector will try to read the specified document and send it as an attachment to the user prompt. Format supported are "doc", "docx", "pdf", ... (see https://tika.apache.org/3.1.0/formats.html)
|null

|*outputJsonSchema*
|false
|The JSON schema that represent how to structure the JSON connector output.
|null

|===

The result can be a simple JSON object or one compliant with a provided JSON schema.
This result will be placed as a map entry of type `java.lang.String` for the key named *output*.

=== Extract connector

This connector allow extracting information from a bonita document.

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*sourceDocumentRef*
|*true*
|The reference to the process document to load and add to the user prompt. If not null, the connector will try to read the specified document  and send it as an attachment to the user prompt. Format supported are "doc", "docx", "pdf", ... (see https://tika.apache.org/3.1.0/formats.html)
|null

|*fieldsToExtract*
|false
|The list of fields to extract from the given document. The connector expect a list of String (like `List.of("firstName","lastName","address")`.
|null

|*outputJsonSchema*
|false
|The JSON schema that represent how to structure the JSON connector output. If a JSON schema is specified, the `fieldsToExtract` parameter is ignored.
|null

|===

IMPORTANT: You must provide at least one of `fieldsToExtract` or `outputJsonSchema` parameters.

The result can be a simple JSON object or one compliant with a provided JSON schema.
This connector result will be placed as a map entry of type `java.lang.String` for the key named *output*.

=== Classify connector

This connector allow to classify a bonita process document according to a list of category provided by the user.

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*sourceDocumentRef*
|*true*
|The reference to the process document to load and add to the user prompt. If not null, the connector will try to read the specified document  and send it as an attachment to the user prompt. Format supported are "doc", "docx", "pdf", ... (see https://tika.apache.org/3.1.0/formats.html)
|null

|*categories*
|*true*
|The list of category used to classify the given document. The connector expect a list of String (like `List.of("RIB","ID",...)`.
It is recommended to add a default category if none other matches such as `Unknown`
|null

|===


The result is a JSON String such as the following sample.

.sample classification result
[source,json]
----
{
  "category": "xxx",
  "confidence": 0.9
}
----

The confidence score is defined as :

- [0.0..0.3]: Very uncertain or guessing
- [0.3..0.6]: Some uncertainty, potential ambiguity exists
- [0.6..0.8]: Reasonably certain, minor doubt
- [0.8..1.0]: Very certain, no doubt

This connector result will be placed as a map entry of type `java.lang.String` for the key named *output*.

== Developing
// _**TODO**_: Here's a brief introduction about what a developer must do in order to start developing the project further:

Prerequisite:

- Java ( **jdk {java-version}** or higher)
- Maven (optional if you chose to use https://github.com/takari/maven-wrapper[maven wrapper script] as archetype option)
- A Git client (optional but highly recommended)
- Docker and docker compose for integration tests

=== Branching strategy

This repository follows the https://gitversion.net/docs/learn/branching-strategies/gitflow/examples[GitFlow branching strategy].


=== Building
// _**TODO**_: If your project needs some additional steps for the developer to build the project after some code changes, state them here:
The project is a standard maven project. For more details about Apache Maven, please refer to the https://maven.apache.org/guides/getting-started/[documentation]

[source,bash,subs=attributes]
----
git clone {uri-repo}.git
cd {project-artifact-id}/
./mwnw package
----

The build should produce connector packages a jar and zip archives under the modules `target/` folders.


=== Run integration tests

// _**TODO**_: Here again you should state what actually happens when the code above gets executed.

The connector needs an OpenAI endpoint up & running. A docker compose file is present in the root folder which starts
a https://ollama.com/[ollama] container that you can use as a local replacement of OpenAI provider.

Just issue `docker compose up -d` and ollama API will be available at `http://localhost:11434/v1`

NOTE: To download model use the following command `docker compose exec ollama bash -c 'ollama pull <model name>'`.  You can check the logs with `docker compose logs -f ollama` for more info.

Once ollama is ready, you can run integration tests using standard maven command and activating a dedicated maven profile (ITs)

`./mvnw verify -PITs`

=== Deploying / Publishing

// _**TODO**_: In case there's some step you have to take that publishes this project to a server, this is the right time to state it.

{doc-url}/managing-extension-studio[Install the connector in your Bonita project using the Studio, window = "_blank"].

==== Release

To release a new version, maintainers may use the Release and Publication GitHub actions.

* Release action will invoke the `gitflow-maven-plugin` to perform all required merges, version updates and tag creation.
* Publication action will build and deploy a given tag to Maven Central.
* A GitHub release should be created and associated with the tag.

// == Contributing
//
// // _**TODO**_: Make easy to your team to jump in and start contributing to your project.
//
// These paragraphs are meant to welcome those kind souls to feel that they are
// needed. You should state something like:
//
// "If you'd like to contribute, please fork the repository and use a feature
// branch. Pull requests are warmly welcome."
//
// If there's anything else the developer needs to know (e.g. the code style
// guide), you should link it here. If there's a lot of things to take into
// consideration, it is common to separate this section to its own file called
// `CONTRIBUTING.adoc` (or similar). If so, you should say that it exists here.

== Links

// _**TODO**_: Even though this information can be found inside the project on machine-readable
// format like in a .json file, it's good to include a summary of most useful
// links to humans using your project. You can include links like:

. Project homepage: {uri-repo}
. Repository: {uri-repo}.git
. Issue tracker: {uri-repo}/issues
// .. In case of sensitive bugs like security vulnerabilities, please contact
//     my@email.com directly instead of using issue tracker. We value your effort
//     to improve the security and privacy of this project!

:doctype: book
:toc: left
:toclevels: 3
:toc: macro
:sectnums:
:icons: font
:source-highlighter: highlightjs
:idprefix:
:idseparator: -
:sectlinks:
:sectanchors:
:linkcss: false

// Vars
:project-group-id: org.bonitasoft.connectors
:project-artifact-id: bonita-connector-ai
:orga: bonitasoft
:uri-org: https://github.com/{orga}
:uri-repo: {uri-org}/{project-artifact-id}
:short-bonita-version: 10.2
:year-bonita-version: 2024.3
:doc-url: https://documentation.bonitasoft.com/bonita/{short-bonita-version}
:java-version: 17
:uri-rel-file-base: {uri-repo}/blob/master
:uri-license: {uri-rel-file-base}/LICENSE

= {project-artifact-id}

image:bonitasoft-community.png[Bonitasoft,link="https://www.bonitasoft.com",width=250px]
image:./openai@2x.png[OpenAI,link="https://openai.com"] image:./mistralai@2x.png[MistralAI,link="https://mistral.ai"] image:https://ollama.com/public/ollama.png[Ollama,link="https://ollama.com",width=100px]

image:{uri-repo}/actions/workflows/build.yaml/badge.svg[Build,link="{uri-repo}/actions?query=build"]
image:https://img.shields.io/github/v/release/{orga}/{project-artifact-id}?color=blue&label=Release[Release,link="{uri-repo}/releases"]
image:https://img.shields.io/maven-central/v/{project-group-id}/{project-artifact-id}.svg?label=Maven%20Central&color=orange[Maven Central,link="https://search.maven.org/search?q=g:%22{project-group-id}%22%20AND%20a:%22{project-artifact-id}%22"]
image:https://sonarcloud.io/api/project_badges/measure?project={orga}_{project-artifact-id}&metric=alert_status[Sonar,link=https://sonarcloud.io/dashboard?id={orga}_{project-artifact-id}]
image:https://img.shields.io/badge/License-GPL%20v2-yellow.svg[License,link="{uri-license}"]

The connectors allow interacting with OpenAI, MistralAI, and Ollama (local LLMs) chat models by sending prompt and documents and returning the generated output.

The Bonita AI Connectors are available for **Bonita {short-bonita-version} Community ({year-bonita-version})** version and above.

'''

toc::[]

'''

== Getting started

To use a connector, add it as a dependency to your Bonita process. Choose the one related to your AI provider.

Currently supported providers:

* image:bonita-connector-ai-openai/src/main/resources/openai.png[OpenAI] OpenAI
* image:bonita-connector-ai-mistral/src/main/resources/mistral.png[MistralAI] Mistral AI
* Ollama (for running local LLMs)

==== OpenAI

[source,xml,subs="attributes+"]
----
<dependency>
    <groupId>org.bonitasoft.connectors</groupId>
    <artifactId>bonita-connector-ai-openai</artifactId>
    <version>x.y.z</version>
</dependency>
----

==== MistralAI

[source,xml,subs="attributes+"]
----
<dependency>
    <groupId>org.bonitasoft.connectors</groupId>
    <artifactId>bonita-connector-ai-mistral</artifactId>
    <version>x.y.z</version>
</dependency>
----

==== Ollama (Local LLMs)

[source,xml,subs="attributes+"]
----
<dependency>
    <groupId>org.bonitasoft.connectors</groupId>
    <artifactId>bonita-connector-ai-ollama</artifactId>
    <version>x.y.z</version>
</dependency>
----

NOTE: Ollama allows you to run large language models locally on your infrastructure. This is ideal for on-premises deployments, data privacy requirements, or cost optimization.

=== Common configuration

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*apiKey*
|false
|The AI provider API key. Parameter is optional for testing purpose but obviously required with official OpenAI endpoint. The connector will use the system environment variable named `AI_API_KEY` or the same JVM property (`-DAI_API_KEY=xxx`) or the provided connector parameter and at last a dummy `changeMe` value.
| changeMe

|*url*
|false
|The AI provider endpoint url. This parameter allows to use an alternate endpoint for tests or custom deployments.
a|
- OpenAI: Official OpenAI endpoint
- MistralAI: Official Mistral AI endpoint
- Ollama: `http://localhost:11434` (default local installation)

|*requestTimeout*
|false
|The request timeout in milliseconds for OpenAI calls.
|null

|*chatModelName*
|false
|The model to use for chat like `gpt-4o`, `gpt-4o-mini`, `llama3.1`, `pixtral`, ... See OpenAI documentation at https://platform.openai.com/docs/models/models, Mistral AI documentation at https://docs.mistral.ai/getting-started/models/models_overview/, and Ollama models at https://ollama.com/library for detailed information about models.
a|
- OpenAI : `gpt-4o`
- MistralAI : `pixtral-12b-2409`
- Ollama : `llama3.1`

|*modelTemperature*
|false
|The temperature to use for the model. Higher values will result in more creative responses. Must be between 0 and 1. Leave blank if the selected model does not support this parameter. If parameter is not present, the temperature will not be set in chat context.
|null
|===


== Connectors

AI connectors have the capability to retrun structured data in JSON format. It is possible to pass a https://json-schema.org/learn/getting-started-step-by-step[JSON schema] to tell the LLM how to format response data.

When using a JSON schema, you **must** list in the `required` property, all the fields you want in the JSON response.

.JSON schema sample
[source, json]
----
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "ProofOfAddress",
  "type": "object",
  "required": [
    "firstName",
    "lastName",
    "fullName",
    "fullAddress",
    "emissionDate",
    "issuerName",
    "identificationNumber"
  ],
  "properties": {
    "firstName": {
      "type": "string"
    },
    "lastName": {
      "type": "string"
    },
    "fullName": {
      "type": "string"
    },
    "fullAddress": {
      "type": "string"
    },
    "emissionDate": {
      "type": "string"
    },
    "issuerName": {
      "type": "string"
    },
    "identificationNumber": {
      "type": "string"
    }
  }
}
----

=== Ask connector

Take a user prompt and send it to OpenAI then return the AI response. The prompt text can ask question about a provided process document.

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*systemPrompt*
|false
|The system prompt to pass to the OpenAI endpoint.  It helps to influence the behavior of the assistant and specify a default context. (ex: You are a personal finance advisor, providing guidance, etc.)
|"You are a polite Assistant"

|*userPrompt*
|*true*
|The user prompt content to send to the AI provider
|

|*sourceDocumentRef*
|false
|The reference to the process document to load and add to the user prompt. If not null, the connector will try to read the specified document and send it as an attachment to the user prompt. Format supported are "doc", "docx", "pdf", ... (see https://tika.apache.org/3.1.0/formats.html)
|null

|*outputJsonSchema*
|false
|The JSON schema that represent how to structure the JSON connector output.
|null

|===

The result can be a simple JSON object or one compliant with a provided JSON schema.
This result will be placed as a map entry of type `java.lang.String` for the key named *output*.

=== Extract connector

This connector allow extracting information from a bonita document.

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*sourceDocumentRef*
|*true*
|The reference to the process document to load and add to the user prompt. If not null, the connector will try to read the specified document  and send it as an attachment to the user prompt. Format supported are "doc", "docx", "pdf", ... (see https://tika.apache.org/3.1.0/formats.html)
|null

|*fieldsToExtract*
|false
|The list of fields to extract from the given document. The connector expect a list of String (like `List.of("firstName","lastName","address")`.
|null

|*outputJsonSchema*
|false
|The JSON schema that represent how to structure the JSON connector output. If a JSON schema is specified, the `fieldsToExtract` parameter is ignored.
|null

|===

IMPORTANT: You must provide at least one of `fieldsToExtract` or `outputJsonSchema` parameters.

The result can be a simple JSON object or one compliant with a provided JSON schema.
This connector result will be placed as a map entry of type `java.lang.String` for the key named *output*.

=== Classify connector

This connector allow to classify a bonita process document according to a list of category provided by the user.

[caption=Configuration]
|===
|Parameter name |Required |Description |Default value

|*sourceDocumentRef*
|*true*
|The reference to the process document to load and add to the user prompt. If not null, the connector will try to read the specified document  and send it as an attachment to the user prompt. Format supported are "doc", "docx", "pdf", ... (see https://tika.apache.org/3.1.0/formats.html)
|null

|*categories*
|*true*
|The list of category used to classify the given document. The connector expect a list of String (like `List.of("RIB","ID",...)`.
It is recommended to add a default category if none other matches such as `Unknown`
|null

|===


The result is a JSON String such as the following sample.

.sample classification result
[source,json]
----
{
  "category": "xxx",
  "confidence": 0.9
}
----

The confidence score is defined as :

- [0.0..0.3]: Very uncertain or guessing
- [0.3..0.6]: Some uncertainty, potential ambiguity exists
- [0.6..0.8]: Reasonably certain, minor doubt
- [0.8..1.0]: Very certain, no doubt

This connector result will be placed as a map entry of type `java.lang.String` for the key named *output*.

== Developing
// _**TODO**_: Here's a brief introduction about what a developer must do in order to start developing the project further:

Prerequisite:

- Java ( **jdk {java-version}** or higher)
- Maven (optional if you chose to use https://github.com/takari/maven-wrapper[maven wrapper script] as archetype option)
- A Git client (optional but highly recommended)
- Docker and docker compose for integration tests

=== Branching strategy

This repository follows the https://gitversion.net/docs/learn/branching-strategies/gitflow/examples[GitFlow branching strategy].


=== Building
// _**TODO**_: If your project needs some additional steps for the developer to build the project after some code changes, state them here:
The project is a standard maven project. For more details about Apache Maven, please refer to the https://maven.apache.org/guides/getting-started/[documentation]

[source,bash,subs=attributes]
----
git clone {uri-repo}.git
cd {project-artifact-id}/
./mwnw package
----

The build should produce connector packages a jar and zip archives under the modules `target/` folders.


=== Run integration tests

Integration tests require actual AI provider endpoints. Here are the options for each provider:

==== Option 1: OpenAI Integration Tests

Set your OpenAI API key and run tests:

[source,bash]
----
export OPENAI_API_KEY=your-api-key-here
./mvnw verify -PITs -pl bonita-connector-ai-openai
----

On Windows PowerShell:
[source,powershell]
----
$env:OPENAI_API_KEY="your-api-key-here"
.\mvnw.cmd verify -PITs -pl bonita-connector-ai-openai
----

==== Option 2: MistralAI Integration Tests

Set your Mistral AI API key and run tests:

[source,bash]
----
export MISTRAL_API_KEY=your-api-key-here
./mvnw verify -PITs -pl bonita-connector-ai-mistral
----

==== Option 3: Ollama Integration Tests (Local - No API Key Required)

Ollama allows running integration tests locally without any API keys or cloud services.

**Step 1: Start Ollama with Docker Compose**

[source,bash]
----
docker-compose -f docker-compose-ollama.yml up -d
----

**Step 2: Pull a model (first time only)**

[source,bash]
----
# For default model (llama3.1 - ~4.7GB)
docker exec -it ollama-test ollama pull llama3.1

# Or for faster testing with smaller model (llama3.2:1b - ~1.3GB)
docker exec -it ollama-test ollama pull llama3.2:1b
----

**Step 3: Verify Ollama is ready**

[source,bash]
----
docker exec -it ollama-test ollama list
----

You should see your pulled model listed.

**Step 4: Run integration tests**

On Linux/Mac:
[source,bash]
----
# Using default model (llama3.1)
./mvnw verify -PITs -pl bonita-connector-ai-ollama

# Using smaller model (llama3.2:1b)
export OLLAMA_MODEL_NAME="llama3.2:1b"
./mvnw verify -PITs -pl bonita-connector-ai-ollama
----

On Windows:
[source,powershell]
----
# Using default model (llama3.1)
.\mvnw.cmd verify -PITs -pl bonita-connector-ai-ollama

# Using smaller model (llama3.2:1b)
$env:OLLAMA_MODEL_NAME="llama3.2:1b"
.\mvnw.cmd verify -PITs -pl bonita-connector-ai-ollama
----

**Step 5: Stop Ollama when done**

[source,bash]
----
docker-compose -f docker-compose-ollama.yml down
----

NOTE: The Ollama Docker volume persists downloaded models between restarts. You only need to pull models once.

**Important: Model Selection for Testing**

Different Ollama models have varying levels of accuracy and capabilities:

* **llama3.1** (~4.7GB): More accurate, better at following instructions, recommended for production use
* **llama3.2:1b** (~1.3GB): Faster and smaller, but may produce less accurate results

The integration tests are designed to be tolerant of model variations. Smaller models may:
- Return variations of expected values (e.g., "Relevé d'Identité Bancaire" instead of "RIB")
- Handle missing data differently (null vs "Absent" vs actual values)
- Have lower confidence scores

These variations are expected and the tests account for them. Choose your model based on your priorities:
- Use **llama3.2:1b** for quick testing and CI/CD pipelines
- Use **llama3.1** or larger for production-grade accuracy

==== Configuration Options for Ollama Tests

You can customize Ollama tests with environment variables:

[caption=Environment Variables]
|===
|Variable |Description |Default

|`OLLAMA_BASE_URL`
|Custom Ollama server URL
|`http://localhost:11434`

|`OLLAMA_MODEL_NAME`
|Specific model to use for tests
|`llama3.1`
|===

Example with custom configuration:
[source,bash]
----
export OLLAMA_BASE_URL="http://remote-server:11434"
export OLLAMA_MODEL_NAME="mistral"
./mvnw verify -PITs -pl bonita-connector-ai-ollama
----

==== Run all integration tests

To run integration tests for all providers at once:

[source,bash]
----
# Make sure Ollama is running and API keys are set
export OPENAI_API_KEY=your-openai-key
export MISTRAL_API_KEY=your-mistral-key
./mvnw verify -PITs
----

=== Deploying / Publishing

// _**TODO**_: In case there's some step you have to take that publishes this project to a server, this is the right time to state it.

{doc-url}/bonita-overview/managing-extension-studio[Install the connector in your Bonita project using the Studio, window = "_blank"].

==== Release

To release a new version, maintainers may use the Release and Publication GitHub Actions workflows.

* Running the https://github.com/bonitasoft/bonita-connector-ai/actions/workflows/release.yaml[Release workflow] will invoke the `gitflow-maven-plugin` to perform all required merges, version updates and tag creation.
* Run the https://github.com/bonitasoft/bonita-connector-ai/actions/workflows/publish.yaml[Publication workflow]  action will build and deploy a given tag to Maven Central.
* A GitHub release should be created and associated with the tag. Manage this manually.

Once this is done, update the https://github.com/bonitasoft/bonita-marketplace[Bonita marketplace repository] with the new version of the connector.

// == Contributing
//
// // _**TODO**_: Make easy to your team to jump in and start contributing to your project.
//
// These paragraphs are meant to welcome those kind souls to feel that they are
// needed. You should state something like:
//
// "If you'd like to contribute, please fork the repository and use a feature
// branch. Pull requests are warmly welcome."
//
// If there's anything else the developer needs to know (e.g. the code style
// guide), you should link it here. If there's a lot of things to take into
// consideration, it is common to separate this section to its own file called
// `CONTRIBUTING.adoc` (or similar). If so, you should say that it exists here.

== Links

// _**TODO**_: Even though this information can be found inside the project on machine-readable
// format like in a .json file, it's good to include a summary of most useful
// links to humans using your project. You can include links like:

. Project homepage: {uri-repo}
. Repository: {uri-repo}.git
. Issue tracker: {uri-repo}/issues
// .. In case of sensitive bugs like security vulnerabilities, please contact
//     my@email.com directly instead of using issue tracker. We value your effort
//     to improve the security and privacy of this project!
